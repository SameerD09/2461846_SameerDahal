{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Submitted by: Sameer Dahal"
      ],
      "metadata": {
        "id": "luKyzuLKFPeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive in Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G26VJKmcE8HU",
        "outputId": "21d675fa-c0fe-4188-9b52-1446c9a2c0e7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required libraries for data manipulation, numerical calculations, plotting, and machine learning\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Additional imports for model evaluation\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "cwpUhMy7FZYh"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1 Implementation from Scratch Step - by - Step Guide:"
      ],
      "metadata": {
        "id": "x9C78tK7Feay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO DO: 1"
      ],
      "metadata": {
        "id": "Q8_IChpqFksX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Read and Observe the Dataset\n",
        "file_path = '/content/drive/My Drive/Dataset/student.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# 2. Print top(5) and bottom(5) rows\n",
        "print(\"Top 5 rows:\")\n",
        "print(data.head())\n",
        "\n",
        "print(\"\\nBottom 5 rows:\")\n",
        "print(data.tail())\n",
        "\n",
        "# 3. Print the Information of Datasets\n",
        "print(\"\\nDataset Information:\")\n",
        "data.info()\n",
        "\n",
        "# 4. Gather Descriptive Statistics\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "print(data.describe())\n",
        "\n",
        "# 5. Split your data into Feature (X) and Label (Y)\n",
        "X = data[['Math', 'Reading']].values  # Features: Math and Reading marks\n",
        "Y = data['Writing'].values  # Target: Writing marks\n",
        "\n",
        "print(\"\\nFeature Matrix (X):\", X[:5])  # First 5 values in Feature matrix\n",
        "print(\"\\nLabel Matrix (Y):\", Y[:5])  # First 5 values in Label matrix\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbmfAzFqF4Bo",
        "outputId": "7cbfda55-cb5c-4813-a4cd-96b6748da557"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 rows:\n",
            "   Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n",
            "\n",
            "Bottom 5 rows:\n",
            "     Math  Reading  Writing\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n",
            "\n",
            "Dataset Information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Math     1000 non-null   int64\n",
            " 1   Reading  1000 non-null   int64\n",
            " 2   Writing  1000 non-null   int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 23.6 KB\n",
            "\n",
            "Descriptive Statistics:\n",
            "              Math      Reading      Writing\n",
            "count  1000.000000  1000.000000  1000.000000\n",
            "mean     67.290000    69.872000    68.616000\n",
            "std      15.085008    14.657027    15.241287\n",
            "min      13.000000    19.000000    14.000000\n",
            "25%      58.000000    60.750000    58.000000\n",
            "50%      68.000000    70.000000    69.500000\n",
            "75%      78.000000    81.000000    79.000000\n",
            "max     100.000000   100.000000   100.000000\n",
            "\n",
            "Feature Matrix (X): [[48 68]\n",
            " [62 81]\n",
            " [79 80]\n",
            " [76 83]\n",
            " [59 64]]\n",
            "\n",
            "Label Matrix (Y): [63 72 78 79 62]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO DO: 2"
      ],
      "metadata": {
        "id": "7tM2uh4_HVEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming we already have the feature matrix (X) and label matrix (Y) from Task 1\n",
        "\n",
        "# 1. Creating the Weight Vector (W)\n",
        "# For simplicity, we can assume W is initialized as a vector of ones or any other values.\n",
        "# Let's initialize it as ones, as it is a simple start for gradient descent.\n",
        "\n",
        "W = np.ones(X.shape[1])  # Number of features (columns) in X determines the size of W\n",
        "\n",
        "# 2. Calculate Y = W^T X\n",
        "# We transpose W and perform the matrix multiplication with X\n",
        "Y_pred = np.dot(X, W)\n",
        "\n",
        "# Print the first 5 predicted values\n",
        "print(\"\\nCalculated Predicted Y (Y_pred) using Y = W^T X:\")\n",
        "print(Y_pred[:5])  # Print the first 5 predictions\n",
        "\n",
        "# Print the shape of the predicted Y to verify the output dimensions\n",
        "print(\"\\nShape of Predicted Y:\", Y_pred.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3WoAhtOHWxB",
        "outputId": "ed546763-5ed8-4c7c-b882-c1e77f5e55a2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculated Predicted Y (Y_pred) using Y = W^T X:\n",
            "[116. 143. 159. 159. 123.]\n",
            "\n",
            "Shape of Predicted Y: (1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO DO: 3"
      ],
      "metadata": {
        "id": "faMSHCpHH-IC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Split the dataset into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Print the shape of the resulting splits\n",
        "print(f\"Training Features shape: {X_train.shape}\")\n",
        "print(f\"Test Features shape: {X_test.shape}\")\n",
        "print(f\"Training Labels shape: {Y_train.shape}\")\n",
        "print(f\"Test Labels shape: {Y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SWqxgfdH_hB",
        "outputId": "398d0630-ceab-43ef-c20a-9a1fc827b1e3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Features shape: (800, 2)\n",
            "Test Features shape: (200, 2)\n",
            "Training Labels shape: (800,)\n",
            "Test Labels shape: (200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.2 Step -2- Build a Cost Function:"
      ],
      "metadata": {
        "id": "fz6eVt1xILKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO DO: 4"
      ],
      "metadata": {
        "id": "DMv2sM1iINXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_function(X, Y, W):\n",
        "    \"\"\"\n",
        "    This function computes the Mean Squared Error.\n",
        "    Parameters:\n",
        "    - X: Feature Matrix\n",
        "    - Y: Target Matrix\n",
        "    - W: Weight Matrix (model parameters)\n",
        "\n",
        "    Returns:\n",
        "    - cost: Mean Squared Error\n",
        "    \"\"\"\n",
        "    m = len(Y)  # Number of training examples\n",
        "    Y_pred = np.dot(X, W)  # Predictions based on the current weights\n",
        "    cost = (1 / (2 * m)) * np.sum((Y_pred - Y) ** 2)  # MSE cost function\n",
        "    return cost\n",
        "\n",
        "# Example test case\n",
        "W_test = np.array([1, 1])  # Example weight values\n",
        "print(f\"Cost function output for test case: {cost_function(X_train, Y_train, W_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4ydRzptIOym",
        "outputId": "cedca60f-9384-4af1-99e2-a81647e19cd2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost function output for test case: 2472.665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO DO: 5"
      ],
      "metadata": {
        "id": "7-IMkBNqInGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Gradient Descent to optimize the weights\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize the parameters (weights) for linear regression.\n",
        "\n",
        "    Parameters:\n",
        "    - X: Feature matrix\n",
        "    - Y: Target vector\n",
        "    - W: Initial weight vector\n",
        "    - alpha: Learning rate\n",
        "    - iterations: Number of gradient descent iterations\n",
        "\n",
        "    Returns:\n",
        "    - W_update: Optimized weights after gradient descent\n",
        "    - cost_history: List of cost values after each iteration\n",
        "    \"\"\"\n",
        "    m = len(Y)  # Number of training examples\n",
        "    cost_history = []\n",
        "\n",
        "    for i in range(iterations):\n",
        "        Y_pred = np.dot(X, W)  # Predicting the values\n",
        "        loss = Y_pred - Y  # Loss (difference between predicted and actual)\n",
        "\n",
        "        # Gradient calculation\n",
        "        dw = (1 / m) * np.dot(X.T, loss)\n",
        "\n",
        "        # Update the weights\n",
        "        W -= alpha * dw\n",
        "\n",
        "        # Calculate and record the cost\n",
        "        cost = cost_function(X, Y, W)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "    return W, cost_history\n",
        "\n",
        "# Test the gradient descent function with example parameters\n",
        "alpha = 0.01\n",
        "iterations = 1000\n",
        "W_initial = np.zeros(X_train.shape[1])  # Initial weights (zeros)\n",
        "W_optimal, cost_history = gradient_descent(X_train, Y_train, W_initial, alpha, iterations)\n",
        "\n",
        "print(f\"Final Weights after Gradient Descent: {W_optimal}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfQhLyaDIoXV",
        "outputId": "b61790d1-280e-41e4-9c63-b9845053909e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights after Gradient Descent: [nan nan]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-981009434.py:14: RuntimeWarning: overflow encountered in square\n",
            "  cost = (1 / (2 * m)) * np.sum((Y_pred - Y) ** 2)  # MSE cost function\n",
            "/tmp/ipython-input-436687984.py:28: RuntimeWarning: invalid value encountered in subtract\n",
            "  W -= alpha * dw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.3 Step -3- Gradient Descent for Simple Linear Regression:"
      ],
      "metadata": {
        "id": "xW_IRrDqIwBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO DO: 6"
      ],
      "metadata": {
        "id": "WVoTzypmIwtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rmse(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This function calculates the Root Mean Square Error (RMSE).\n",
        "\n",
        "    Parameters:\n",
        "    - Y: Actual target values\n",
        "    - Y_pred: Predicted target values\n",
        "\n",
        "    Returns:\n",
        "    - rmse: Root Mean Square Error\n",
        "    \"\"\"\n",
        "    rmse_value = np.sqrt(np.mean((Y_pred - Y) ** 2))\n",
        "    return rmse_value\n",
        "\n",
        "# Test RMSE on the test set predictions\n",
        "Y_pred_test = np.dot(X_test, W_optimal)\n",
        "model_rmse = rmse(Y_test, Y_pred_test)\n",
        "print(f\"RMSE on Test Set: {model_rmse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8KniOmjIzb7",
        "outputId": "a50a5f6d-5cd3-48b0-deaa-46ff30939e0d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE on Test Set: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO DO: 7"
      ],
      "metadata": {
        "id": "EGvhnyx1I6hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def r2(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This function calculates the R-Squared (Coefficient of Determination).\n",
        "\n",
        "    Parameters:\n",
        "    - Y: Actual target values\n",
        "    - Y_pred: Predicted target values\n",
        "\n",
        "    Returns:\n",
        "    - r2: R-squared value\n",
        "    \"\"\"\n",
        "    ss_tot = np.sum((Y - np.mean(Y)) ** 2)\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "    r2_value = 1 - (ss_res / ss_tot)\n",
        "    return r2_value\n",
        "\n",
        "# Test R² on the test set predictions\n",
        "model_r2 = r2(Y_test, Y_pred_test)\n",
        "print(f\"R-Squared on Test Set: {model_r2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edBVFBqsI79y",
        "outputId": "5382f10a-40a8-4e7e-d902-47a39de7fb38"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-Squared on Test Set: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.4 Step -4- Evaluate the Model:"
      ],
      "metadata": {
        "id": "QqaE9Xk3JB5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO DO: 8"
      ],
      "metadata": {
        "id": "S3mkPS_tJDVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Step 1: Load the dataset\n",
        "    data = pd.read_csv('/content/drive/My Drive/Dataset/student.csv')\n",
        "\n",
        "    # Step 2: Split into features and label\n",
        "    X = data[['Math', 'Reading']].values\n",
        "    Y = data['Writing'].values\n",
        "\n",
        "    # Step 3: Split into train and test sets (80% train, 20% test)\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Step 4: Initialize weights, learning rate, and number of iterations\n",
        "    W_initial = np.zeros(X_train.shape[1])\n",
        "    alpha = 0.01\n",
        "    iterations = 1000\n",
        "\n",
        "    # Step 5: Run gradient descent\n",
        "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W_initial, alpha, iterations)\n",
        "\n",
        "    # Step 6: Predict on the test set\n",
        "    Y_pred = np.dot(X_test, W_optimal)\n",
        "\n",
        "    # Step 7: Evaluate using RMSE and R²\n",
        "    model_rmse = rmse(Y_test, Y_pred)\n",
        "    model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "    # Step 8: Output results\n",
        "    print(\"Final Weights:\", W_optimal)\n",
        "    print(\"RMSE on Test Set:\", model_rmse)\n",
        "    print(\"R-Squared on Test Set:\", model_r2)\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zm8dbYCHJFGf",
        "outputId": "1e57c073-5632-4ddf-b945-71bfb1c22286"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [nan nan]\n",
            "RMSE on Test Set: nan\n",
            "R-Squared on Test Set: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-981009434.py:14: RuntimeWarning: overflow encountered in square\n",
            "  cost = (1 / (2 * m)) * np.sum((Y_pred - Y) ** 2)  # MSE cost function\n",
            "/tmp/ipython-input-436687984.py:28: RuntimeWarning: invalid value encountered in subtract\n",
            "  W -= alpha * dw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO DO: 9"
      ],
      "metadata": {
        "id": "ZV2pLchhJOYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To-Do 9: Model Evaluation and Overfitting Analysis\n",
        "\n",
        "# To check for overfitting or underfitting:\n",
        "# - If RMSE is high, the model might be underfitting.\n",
        "# - If RMSE is low on the training set but high on the test set, the model might be overfitting.\n",
        "\n",
        "# Experiment with different learning rates (alpha values) and observe the results.\n",
        "# Example:\n",
        "alpha_high = 0.1\n",
        "alpha_low = 0.0001\n",
        "W_high, _ = gradient_descent(X_train, Y_train, W_initial, alpha_high, iterations)\n",
        "W_low, _ = gradient_descent(X_train, Y_train, W_initial, alpha_low, iterations)\n",
        "\n",
        "# Check the cost (RMSE) for different alphas on the test set\n",
        "print(f\"RMSE with High Alpha (0.1): {rmse(Y_test, np.dot(X_test, W_high))}\")\n",
        "print(f\"RMSE with Low Alpha (0.0001): {rmse(Y_test, np.dot(X_test, W_low))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gf_V-iunJVbV",
        "outputId": "8744a599-2737-46d3-9766-0e0298649101"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE with High Alpha (0.1): nan\n",
            "RMSE with Low Alpha (0.0001): nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.5 Step -5- Main Function to Integrate All Steps:"
      ],
      "metadata": {
        "id": "nqzEnVKQJerT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO DO: 10"
      ],
      "metadata": {
        "id": "y40Ap2xuJfZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To-Do 10: Main Function to Integrate All Steps\n",
        "\n",
        "def main():\n",
        "    # Step 1: Load the dataset\n",
        "    data = pd.read_csv('/content/drive/My Drive/Dataset/student.csv')\n",
        "\n",
        "    # Step 2: Split into features (X) and label (Y)\n",
        "    X = data[['Math', 'Reading']].values  # Features: Math and Reading marks\n",
        "    Y = data['Writing'].values  # Target: Writing marks\n",
        "\n",
        "    # Step 3: Split into train and test sets (80% train, 20% test)\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Step 4: Initialize weights (W) as zeros, learning rate, and number of iterations\n",
        "    W_initial = np.zeros(X_train.shape[1])  # Initial weights (zeros)\n",
        "    alpha = 0.01  # Learning rate\n",
        "    iterations = 1000  # Number of iterations for gradient descent\n",
        "\n",
        "    # Step 5: Perform Gradient Descent\n",
        "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W_initial, alpha, iterations)\n",
        "\n",
        "    # Step 6: Make predictions on the test set\n",
        "    Y_pred = np.dot(X_test, W_optimal)\n",
        "\n",
        "    # Step 7: Evaluate the model using RMSE and R-Squared\n",
        "    model_rmse = rmse(Y_test, Y_pred)\n",
        "    model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "    # Step 8: Output the results\n",
        "    print(\"Final Weights:\", W_optimal)\n",
        "    print(\"RMSE on Test Set:\", model_rmse)\n",
        "    print(\"R-Squared on Test Set:\", model_r2)\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNYJSqN9JeB9",
        "outputId": "89f26da3-852d-43e0-ff79-83768bd5fe32"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [nan nan]\n",
            "RMSE on Test Set: nan\n",
            "R-Squared on Test Set: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "/tmp/ipython-input-981009434.py:14: RuntimeWarning: overflow encountered in square\n",
            "  cost = (1 / (2 * m)) * np.sum((Y_pred - Y) ** 2)  # MSE cost function\n",
            "/tmp/ipython-input-436687984.py:28: RuntimeWarning: invalid value encountered in subtract\n",
            "  W -= alpha * dw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO DO: 11"
      ],
      "metadata": {
        "id": "5OExNhEYKJI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X_train, Y_train, X_test, Y_test, and other necessary variables have been defined\n",
        "\n",
        "# Evaluate the model performance on the training set\n",
        "Y_pred_train = np.dot(X_train, W_optimal)\n",
        "train_rmse = rmse(Y_train, Y_pred_train)\n",
        "train_r2 = r2(Y_train, Y_pred_train)\n",
        "\n",
        "# Print findings for training set\n",
        "print(\"\\nTraining Performance:\")\n",
        "print(f\"RMSE on Training Set: {train_rmse}\")\n",
        "print(f\"R-Squared on Training Set: {train_r2}\")\n",
        "\n",
        "# Check for overfitting or underfitting based on RMSE and R2 values\n",
        "if train_rmse < model_rmse and train_r2 > model_r2:\n",
        "    print(\"The model might be overfitting (low RMSE and high R² on training, high RMSE and low R² on test).\")\n",
        "elif train_rmse > model_rmse or train_r2 < model_r2:\n",
        "    print(\"The model might be underfitting (high RMSE and low R² on both training and test).\")\n",
        "else:\n",
        "    print(\"The model has acceptable performance (similar RMSE and R² on both training and test).\")\n",
        "\n",
        "# Experiment with different learning rates (alpha values)\n",
        "alpha_high = 0.1  # High learning rate\n",
        "alpha_low = 0.0001  # Low learning rate\n",
        "\n",
        "# Initialize weights to zeros for consistency across experiments\n",
        "W_initial = np.zeros(X_train.shape[1])\n",
        "\n",
        "# Test with high learning rate\n",
        "W_high, _ = gradient_descent(X_train, Y_train, W_initial, alpha_high, iterations)\n",
        "Y_pred_high = np.dot(X_test, W_high)\n",
        "rmse_high = rmse(Y_test, Y_pred_high)\n",
        "\n",
        "# Test with low learning rate\n",
        "W_low, _ = gradient_descent(X_train, Y_train, W_initial, alpha_low, iterations)\n",
        "Y_pred_low = np.dot(X_test, W_low)\n",
        "rmse_low = rmse(Y_test, Y_pred_low)\n",
        "\n",
        "# Print RMSE values for different learning rates\n",
        "print(f\"\\nRMSE with High Alpha (0.1): {rmse_high}\")\n",
        "print(f\"RMSE with Low Alpha (0.0001): {rmse_low}\")"
      ],
      "metadata": {
        "id": "r-J9gzk_KKc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5cc3711-f946-42ab-8bbf-1a6cbffd00f8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Performance:\n",
            "RMSE on Training Set: nan\n",
            "R-Squared on Training Set: nan\n",
            "The model has acceptable performance (similar RMSE and R² on both training and test).\n",
            "\n",
            "RMSE with High Alpha (0.1): nan\n",
            "RMSE with Low Alpha (0.0001): nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-981009434.py:14: RuntimeWarning: overflow encountered in square\n",
            "  cost = (1 / (2 * m)) * np.sum((Y_pred - Y) ** 2)  # MSE cost function\n",
            "/tmp/ipython-input-436687984.py:28: RuntimeWarning: invalid value encountered in subtract\n",
            "  W -= alpha * dw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Performance: The training RMSE and R² are calculated to check if the model is overfitting or underfitting.\n",
        "\n",
        "Overfitting and Underfitting: We compare the RMSE on the training and test sets to determine if the model is overfitting or underfitting.\n",
        "\n",
        "Learning Rate Experimentation: The code tests the effect of different learning rates (alpha) on the model’s RMSE and prints the results. A higher learning rate might lead to poor convergence, while a lower learning rate might cause slower learning."
      ],
      "metadata": {
        "id": "A-HWpoyQKLik"
      }
    }
  ]
}